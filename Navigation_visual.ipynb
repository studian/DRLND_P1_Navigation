{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana Navigation\n",
    "\n",
    "---\n",
    "This is the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "this code is implemented on Window 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from dqn_agent import Agent\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 0\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"unity/VisualBanana_Linux/Banana.x86\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd38dd0c3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States have shape: (1, 84, 84, 3)\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.visual_observations[0]\n",
    "print('States look like:')\n",
    "plt.imshow(np.squeeze(state))\n",
    "plt.show()\n",
    "state_size = state.shape\n",
    "print('States have shape:', state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. define the DQN\n",
    "\n",
    "dqn execute DQN algorithm according to agent mode. you can choose one from `vanilla`, `double`, `prioritized`, `dueling`, `rainbow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(agent, n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "        state = env_info.visual_observations[0]            # get the current state\n",
    "        score = 0                                          # initialize the score\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps).astype(int)     # select an action\n",
    "            #np.int64 raise error AttributeError: 'numpy.int64' object has no attribute 'keys'\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.visual_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward                                # update the score\n",
    "            state = next_state                             # roll over the state to next time step\n",
    "            if done:                                       # exit loop if episode finished\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.10\n",
      "Episode 200\tAverage Score: -0.14\n",
      "Episode 300\tAverage Score: -0.02\n",
      "Episode 400\tAverage Score: 0.041\n",
      "Episode 500\tAverage Score: -0.03\n",
      "Episode 600\tAverage Score: 0.012\n",
      "Episode 700\tAverage Score: -0.09\n",
      "Episode 800\tAverage Score: 0.351\n",
      "Episode 900\tAverage Score: -0.03\n",
      "Episode 1000\tAverage Score: 0.00\n",
      "Episode 1100\tAverage Score: 0.011\n",
      "Episode 1200\tAverage Score: 0.041\n",
      "Episode 1300\tAverage Score: -0.01\n",
      "Episode 1400\tAverage Score: -0.07\n",
      "Episode 1500\tAverage Score: -0.02\n",
      "Episode 1600\tAverage Score: -0.09\n",
      "Episode 1700\tAverage Score: 0.122\n",
      "Episode 1800\tAverage Score: 0.002\n",
      "Episode 1900\tAverage Score: 0.041\n",
      "Episode 2000\tAverage Score: -0.09\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=37, action_size=4, seed=0, mode=\"vanilla\", visual=True)\n",
    "double_scores = dqn(agent)\n",
    "\n",
    "with open('saved_scores/vanilla_visual_scores.txt', 'wb') as f:\n",
    "    pickle.dump(double_scores, f)\n",
    "agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 4: Index tensor must have same dimensions as input tensor at /pytorch/aten/src/THC/generic/THCTensorScatterGather.cu:16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-048fd05de332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m37\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"double\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdouble_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_scores/double_visual_scores.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdouble_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8bf8954ea9cb>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(agent, n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                   \u001b[0;31m# get the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                  \u001b[0;31m# see if episode has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m                                \u001b[0;31m# update the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m                             \u001b[0;31m# roll over the state to next time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hdd1/gdh/udacity/DRLND_P1_Navigation/dqn_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hdd1/gdh/udacity/DRLND_P1_Navigation/dqn_agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mnext_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Compute Q targets for current states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 4: Index tensor must have same dimensions as input tensor at /pytorch/aten/src/THC/generic/THCTensorScatterGather.cu:16"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=37, action_size=4, seed=0, mode=\"double\", visual=True)\n",
    "double_scores = dqn(agent)\n",
    "\n",
    "with open('saved_scores/double_visual_scores.txt', 'wb') as f:\n",
    "    pickle.dump(double_scores, f)\n",
    "agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=37, action_size=4, seed=0, mode=\"prioritized\", visual=True)\n",
    "prioritized_scores, prioritized_scores_window = dqn(agent)\n",
    "\n",
    "with open('saved_scores/prioritized_visual_scores.txt', 'wb') as f:\n",
    "    pickle.dump(prioritized_scores, f)\n",
    "agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=37, action_size=4, seed=0, mode=\"dueling\", visual=True)\n",
    "dueling_scores, dueling_scores_window = dqn(agent)\n",
    "\n",
    "with open('saved_scores/dueling_visual_scores.txt', 'wb') as f:\n",
    "    pickle.dump(dueling_scores, f)\n",
    "agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=37, action_size=4, seed=0, mode=\"rainbow\", visual=True)\n",
    "rainbow_scores, rainbow_scores_window = dqn(agent)\n",
    "\n",
    "with open('saved_scores/rainbow_visual_scores.txt', 'wb') as f:\n",
    "    pickle.dump(rainbow_scores, f)\n",
    "agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla, = plt.plot(vanilla_scores, label='vanilla')\n",
    "double, = plt.plot(double_scores, label='double')\n",
    "prioritized, = plt.plot(prioritized_scores, label='prioritized')\n",
    "dueling, = plt.plot(dueling_scores, label='dueling')\n",
    "rainbow, = plt.plot(rainbow_scores, label='rainbow')\n",
    "plt.legend(handles=[vanilla, double, prioritized, dueling, rainbow])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean(scores):\n",
    "    rolling_mean = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    for i in range(len(scores)):\n",
    "        scores_window.append(scores[i])\n",
    "        rolling_mean.append(np.mean(scores_window))\n",
    "    return rolling_mean\n",
    "vanila_rolling_mean = rolling_mean(vanilla_scores)\n",
    "double_rolling_mean = rolling_mean(double_scores)\n",
    "prioritized_rolling_mean = rolling_mean(prioritized_scores)\n",
    "dueling_rolling_mean = rolling_mean(dueling_scores)\n",
    "rainbow_rolling_mean = rolling_mean(rainbow_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla, = plt.plot(vanila_rolling_mean, label='vanilla')\n",
    "double, = plt.plot(double_rolling_mean, label='double')\n",
    "prioritized, = plt.plot(prioritized_rolling_mean, label='prioritized')\n",
    "dueling, = plt.plot(dueling_rolling_mean, label='dueling')\n",
    "rainbow, = plt.plot(rainbow_rolling_mean, label='rainbow')\n",
    "plt.legend(handles=[vanilla, double, prioritized, dueling, rainbow])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
